{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fairlearn\n",
    "#https://fairlearn.github.io/api_reference/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment, Run, Workspace\n",
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "print(ws.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap.datasets import adult  # shap is only used its dataset utility\n",
    "adult()[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#If the person is elegable for a loan\n",
    "pd.DataFrame(adult()[1]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the function definition\n",
    "import inspect\n",
    "print(inspect.getsource(adult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_true = adult(display=True) #Raw dataset\n",
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## We do not want sex or race affecting the model so we remove them to avoid any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw, y_true = adult(display=False) #Factorize the cats\n",
    "#'remove 'Sex' and 'Race' from the model so they do not cause bias \n",
    "X = X_raw.drop(labels=['Sex','Race'], axis=1)\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert y to binary 1 or 0\n",
    "y_true = y_true * 1 #convert y to bianry numeric\n",
    "pd.DataFrame(y_true).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from fairlearn.metrics import group_summary\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#experiment = Experiment(workspace=ws, name=\"FairlearnDemo1\") \n",
    "#run = experiment.start_logging(snapshot_directory=None) #Don't create a snapshot\n",
    "\n",
    "#Split the data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "#Train a tree\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X, y_true)\n",
    "\n",
    "#Run the predictions\n",
    "y_pred = classifier.predict(X)\n",
    "\n",
    "#See the accuracy\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "#run.log('Accuracy', metrics.accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade scikit-learn\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "#print(confusion_matrix(y_true, y_pred))\n",
    "#plot_confusion_matrix(classifier, X, y_true)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "#run.log_image(\"ConfusionMatrix\", path=None, plot=plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Bring the sex column into a dataset so we can leter check any bias that has worked its way into the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = X_raw['Sex'].apply(lambda sex: \"female\" if sex == 0 else \"male\") #convert sex to male/female\n",
    "pd.DataFrame(sex).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## We can see in the original data we had more males then females "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dfgroup = pd.DataFrame(sex).groupby(['Sex'])['Sex'].count()\n",
    "plt.bar(['female','male'], dfgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#print('Accuracy of the model between females and males')\n",
    "#group_summary(accuracy_score, y_true, y_pred, sensitive_features=sex)\n",
    "\n",
    "#Model has greater accuracy for females\n",
    "#'female': 0.9958221149382601, 'male': 0.9668196420376319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fairlearn.metrics import selection_rate_group_summary\n",
    "##Percent of data points marked as True \n",
    "#print('female and male chance of getting a loan')\n",
    "#selection_rate_group_summary(y_true, y_pred, sensitive_features=sex)\n",
    "##Males have a higher percentage chance of getting a loan\n",
    "##'female': 0.1065824900194968, 'male': 0.28347865993575033\n",
    "##Bias in previous loans entering the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fairlearn Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features=sex,\n",
    "                       sensitive_feature_names=['sex'],\n",
    "                       y_true=y_true,\n",
    "                       y_pred={\"initial model\": y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /\\ is the disparity on who will get a loan a problem? We will need ot look at explinations to understand more about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show with regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionageX = X\n",
    "age = regressionageX['Age']\n",
    "regressionageX = regressionageX.drop(labels=['Age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Train\n",
    "clin = LinearRegression()\n",
    "clin.fit(regressionageX, age)\n",
    "\n",
    "#Run the predictions\n",
    "y_age_pred = clin.predict(regressionageX)\n",
    "\n",
    "#See the accuracy\n",
    "#metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(mean_squared_error(age, y_age_pred, multioutput='raw_values'))\n",
    "print(r2_score(age, y_age_pred))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(age.sort_values(0).values, label = \"TrueAge\")\n",
    "ax.plot(pd.DataFrame(y_age_pred).sort_values(0).values, label = \"PredAge\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual vs predicted\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(age.head(50), pd.DataFrame(age).head(50), 'o')\n",
    "\n",
    "#Actual vs predicted\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(age.head(50), pd.DataFrame(y_age_pred).head(50), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features=sex.head(500),\n",
    "                       sensitive_feature_names=['sex'],\n",
    "                       y_true=age.head(500),\n",
    "                       y_pred={\"initial model\": pd.DataFrame(y_age_pred).head(500)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more features to investigate (race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the race details\n",
    "Xrace, delme = adult(display=True) #Raw dataset\n",
    "Xrace = Xrace[['Race']]\n",
    "Xrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_summary, selection_rate_group_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Accuracy by race\n",
    "group_summary(accuracy_score, y_true, y_pred, sensitive_features=Xrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Percentage of by race\n",
    "selection_rate_group_summary(y_true, y_pred, sensitive_features=Xrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put sex and reace into one dataframe\n",
    "sexrace = pd.concat([sex, Xrace], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features=sexrace,\n",
    "                       sensitive_feature_names=['sex', 'race'],\n",
    "                       y_true=y_true,\n",
    "                       y_pred={\"initial model\": y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the mitigator to get a less bias model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, GridSearch\n",
    "constraint = DemographicParity()\n",
    "classifier = DecisionTreeClassifier()\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X, y_true, sensitive_features=sex)\n",
    "y_pred_mitigated = mitigator.predict(X)\n",
    "selection_rate_group_summary(y_true, y_pred_mitigated, sensitive_features=sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View the results\n",
    "FairlearnDashboard(sensitive_features=sex,\n",
    "                       sensitive_feature_names=['sex'],\n",
    "                       y_true=y_true,\n",
    "                       y_pred={\"initial model\": y_pred, \"mitigated model\": y_pred_mitigated})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, GridSearch\n",
    "mitigatorExponentiatedGradient = ExponentiatedGradient(DecisionTreeClassifier(), DemographicParity())\n",
    "mitigatorExponentiatedGradient.fit(X, y_true, sensitive_features=sex)\n",
    "y_pred_mitigatedExponentiatedGradient = mitigatorExponentiatedGradient.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mitigatorGridSearch = GridSearch(DecisionTreeClassifier(), DemographicParity())\n",
    "mitigatorGridSearch.fit(X, y_true, sensitive_features=sex)\n",
    "y_pred_mitigatedGridSearch = mitigatorGridSearch.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a basic logistic regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "lg.fit(X, y_true)\n",
    "y_pred_lg = lg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(20,20,20), activation='relu', solver='adam', max_iter=500)\n",
    "clf.fit(X, y_true)\n",
    "y_pred_MLP = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FairlearnDashboard(sensitive_features=sex,\n",
    "                       sensitive_feature_names=['sex'],\n",
    "                       y_true=y_true,\n",
    "                       y_pred={\"initial model\": y_pred, \"mitigated model Exponentiated Gradient\": y_pred_mitigatedExponentiatedGradient, \"Logistic Regression\": y_pred_lg, \"NN\": y_pred_MLP})"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
